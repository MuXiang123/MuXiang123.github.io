---
title: 极客时间《netty源码剖析与实战》学习笔记2
date: 2023-06-21 21:03:58
tags:
 - Netty
 - 极客时间
 - 源码
 - Java
categories:
 - [极客时间, Netty源码剖析与实战]
 - [Netty]
---

## **7.Netty的锁**

### **分析同步问题的三要素**

- 原子性，线程切换带来原子性问题
- 可见性，缓存带来的可见性问题
- 有序性，编译优化产生有序性问题

### **锁的分类**

- 对竞争的态度：乐观锁（java.util.concurrent 包中的原子类）、悲观锁（Synchronized）
- 对等待锁是否公平：公平锁 new ReentrantLock (true)、非公平锁 new ReentrantLock ()
- 是否可以共享：共享锁与独享锁：ReadWriteLock ，其读锁是共享锁，其写锁是独享

## **Netty中使用锁的五个关键点**

### **注意锁的范围和->减少粒度**

初始化 channel （io.netty.bootstrap.ServerBootstrap#init） Synchronized method -> Synchronized block 不是将整个方法加锁，而是在代码块上加锁。

对于option0和attrs0两个不会出现线程安全问题的代码不加锁。

![img](https://pic1.zhimg.com/80/v2-24033f5c31a99b059aed60d35260ce08_720w.webp)

### **注意锁对象本身的大小->减少空间占用**

统计待发送的字节数（io.netty.channel.ChannelOutboundBuffer AtomicLong -> Volatile long + AtomicLongFieldUpdater

![img](https://pic1.zhimg.com/80/v2-ece282236aabb36fc72e58da1a537504_720w.webp)

看似代码变复杂了，而且没用。但是从对象所占用的字节数方面来看：

Atomic long VS long：

前者是一个对象，包含对象头（object header）以用来保存 hashcode、lock 等信息，32 位系统占 用8字节；64 位系统占 16 字节，所以在 64 位系统情况下：

- volatile long = 8 bytes
- AtomicLong = 8 bytes （volatile long）+ 16bytes （对象头）+ 8 bytes (引用，不一定是8字节，因为会存在指针压缩等优化) = 32 bytes

至少节约 24 字节!

> 结论：Atomic* objects -> Volatile primary type + Static Atomic*FieldUpdater

### **注意锁的速度->提高并发性**

netty可以根据不同情况，选择不同的并发包实现。

当JDK < 1.8时，使用ConcurrentHashMapV8（ConcurrentHashMap 在 JDK8 中的版本）jdk8的实现拷贝到了netty中（现在netty中没有了）。

![img](https://pic4.zhimg.com/80/v2-032a9d1466742df6f8fe5ce13087058b_720w.webp)

### **不同场景下选择不同的并发包->因需而变**

1. 关闭和等待关闭事件执行器（Event Execute）

Object.wait/notify -> CountDownLatch

io.netty.util.concurrent.SingleThreadEventExecutor#threadLock：

![img](https://pic2.zhimg.com/80/v2-c806e34d4d7e9f8667b0fc60a9d6bb19_720w.webp)

**Object.wait/notify** 需要把要执行的语句放到一个监视器中。

1. Nio Event loop中负责存储task的
   Queue Jdk’s LinkedBlockingQueue **(MPMC多生产者多消费者模式**) -> jctools’ **MPSC(多生产者单消费者模式)** io.netty.util.internal.PlatformDependent.Mpsc#newMpscQueue(int)：

对于Event loop来说，它只绑定了一个线程，所以只有一个消费者。

说明优化的基础是建立在具体的场景上的。

### **衡量好锁的价值->能不用则不用**

1. 局部串行：netty对于每个连接而言，都是一个串行化操作。Channel 的 I/O 请求处理 Pipeline 是串行的

![img](https://pic2.zhimg.com/80/v2-aa171f49bd974476c51f632729e259a9_720w.webp)



1. 整体并行：多个串行化的线程（NioEventLoop）

![img](https://pic1.zhimg.com/80/v2-53dde61eb68bc34d7e595421281a3d0c_720w.webp)

Netty 应用场景下：局部串行 + 整体并行 > 一个队列 + 多个线程模式:

- 降低用户开发难度、逻辑简单、提升处理性能
- 避免锁带来的上下文切换和并发保护等额外开销

1. 避免用锁： 用 ThreadLocal 来避免资源争用，例如 Netty 轻量级的线程池实

## **8.Netty如何玩转内存使用**

内存使用的目标：

- 内存占用少
- 应用速度快

对于java而言:减少Full GC的STWW（Stop the world）时间

### **减少对像本身大小**

- 用基本类型就不要用包装类
- 应该定义成类变量的不要定义为实例变量

### **对分配内存进行预估**

- 对于已经可以预知固定 size 的 HashMap避免扩容 可以提前计算好初始size或者直接使用

com.google.common.collect.Maps#newHashMapWithExpectedSize

```java
    public static <K, V> HashMap<K, V> newHashMapWithExpectedSize(int expectedSize) {
        return new HashMap(capacity(expectedSize));
    }

    static int capacity(int expectedSize) {
        if (expectedSize < 3) {
            CollectPreconditions.checkNonnegative(expectedSize, "expectedSize");
            return expectedSize + 1;
        } else {
            return expectedSize < 1073741824 ? (int)((float)expectedSize / 0.75F + 1.0F) : 2147483647;
        }
    }
```

- Netty 根据接受到的数据动态调整（guess）下个要分配的 Buffer 的大小。可参考 io.netty.channel.AdaptiveRecvByteBufAllocator

```java
        /**
         * 接受数据buffer的容量会尽可能的足够大以接受数据
         * 也尽可能的小以不会浪费它的空间
         * @param actualReadBytes
         */
private void record(int actualReadBytes) {
    //尝试是否可以减小分配的空间仍然能满足需求：
    //尝试方法：当前实际读取的size是否小于或等于打算缩小的尺寸
    if (actualReadBytes <= SIZE_TABLE[max(0, index - INDEX_DECREMENT - 1)]) {
        //decreaseNow: 连续2次尝试减小都可以
        if (decreaseNow) {
            //减小
            index = max(index - INDEX_DECREMENT, minIndex);
            nextReceiveBufferSize = SIZE_TABLE[index];
            decreaseNow = false;
        } else {
            decreaseNow = true;
        }
        //判断是否实际读取的数据大于等于预估的，如果是，尝试扩容
    } else if (actualReadBytes >= nextReceiveBufferSize) {
        index = min(index + INDEX_INCREMENT, maxIndex);
        nextReceiveBufferSize = SIZE_TABLE[index];
        decreaseNow = false;
    }
}
```

### **Zero-Copy 零复制**

使用逻辑组合，代替实际复制。 例如 CompositeByteBuf： io.netty.handler.codec.ByteToMessageDecoder#COMPOSITE_CUMULATOR 避免了内存的复制而将数据组合到一起。

```java
    public static final Cumulator COMPOSITE_CUMULATOR = new Cumulator() {
        @Override
        public ByteBuf cumulate(ByteBufAllocator alloc, ByteBuf cumulation, ByteBuf in) {
            ByteBuf buffer;
            try {
                if (cumulation.refCnt() > 1) {
                    // Expand cumulation (by replace it) when the refCnt is greater then 1 which may happen when the
                    // user use slice().retain() or duplicate().retain().
                    //
                    // See:
                    // - https://github.com/netty/netty/issues/2327
                    // - https://github.com/netty/netty/issues/1764
                    buffer = expandCumulation(alloc, cumulation, in.readableBytes());
                    buffer.writeBytes(in);
                } else {
                    CompositeByteBuf composite;

                    //创建composite bytebuf，如果已经创建过，就不用了
                    if (cumulation instanceof CompositeByteBuf) {
                        composite = (CompositeByteBuf) cumulation;
                    } else {
                        composite = alloc.compositeBuffer(Integer.MAX_VALUE);
                        composite.addComponent(true, cumulation);
                    }
                    //避免内存复制
                    composite.addComponent(true, in);
                    in = null;
                    buffer = composite;
                }
                return buffer;
            } finally {
                if (in != null) {
                    // We must release if the ownership was not transferred as otherwise it may produce a leak if
                    // writeBytes(...) throw for whatever release (for example because of OutOfMemoryError).
                    in.release();
                }
            }
        }
    };
```

- 使用包装，代替实际复制。

```java
byte[] bytes = data.getBytes();  

ByteBuf byteBuf = Unpooled.wrappedBuffer(bytes)
```

- 调用 JDK 的 Zero-Copy 接口。 Netty 中也通过在 DefaultFileRegion 中包装了 NIO 的 FileChannel.transferTo() 方法实 现了零拷贝：io.netty.channel.DefaultFileRegion#transferTo

```java
    public long transferTo(WritableByteChannel target, long position) throws IOException {
        long count = this.count - position;
        if (count < 0 || position < 0) {
            throw new IllegalArgumentException(
                    "position out of range: " + position +
                    " (expected: 0 - " + (this.count - 1) + ')');
        }
        if (count == 0) {
            return 0L;
        }
        if (refCnt() == 0) {
            throw new IllegalReferenceCountException(0);
        }
        // Call open to make sure fc is initialized. This is a no-oop if we called it before.
        open();

        long written = file.transferTo(this.position + position, count, target);
        if (written > 0) {
            transferred += written;
        } else if (written == 0) {
            // If the amount of written data is 0 we need to check if the requested count is bigger then the
            // actual file itself as it may have been truncated on disk.
            //
            // See https://github.com/netty/netty/issues/8868
            validate(this, position);
        }
        return written;
    }
```

### **堆外内存**

堆外内存生活场景：

- 夏日，小区周边的烧烤店铺，人满为患坐不下，店家常常怎么办？

- 解决思路：店铺门口摆很多桌子招待客人。

- - 店内 -> JVM 内部 -> 堆（heap) + 非堆（non heap）
  - 店外 -> JVM 外部 -> 堆外（off heap）

优点：

- 更广阔的“空间 ”，缓解店铺内压力 -> 破除堆空间限制，减轻 GC 压力
- 减少“冗余”细节（假设烧烤过程为了气氛在室外进行：烤好直接上桌：vs 烤好还 要进店内）-> 避免复制

缺点：

- 需要搬桌子 -> 创建速度稍慢
- 受城管管、风险大 -> 堆外内存受操作系统管理

### **内存池**

**点菜单的演进：**

• 一张纸：一桌客人一张纸

• 点菜平板：循环使用

**为什么引入对象池：**

• 创建对象开销大

• 对象高频率创建且可复用

• 支持并发又能保护系统

• 维护、共享有限的资源

**如何实现对象池？**

• 开源实现：Apache Commons Pool （全局共享，资源争用比较厉害）

• Netty 轻量级对象池实现 io.netty.util.Recycler （基于netty场景实现）

## **源码解读Netty内存使用**

### **内存池/非内存池的默认选择及切换方式？**

以io.netty.allocator.type为准，没有的话，安卓平台用非池化实现，其他用池化实现

**默认使用池化的实现**，可以使用io.netty.allocator.type设置池化或者非池化的实现。 io.netty.channel.DefaultChannelConfig#allocator

```java
//默认bytebuf分配器
private volatile ByteBufAllocator allocator = ByteBufAllocator.DEFAULT;
```

io.netty.buffer.ByteBufAllocator

```java
ByteBufAllocator DEFAULT = ByteBufUtil.DEFAULT_ALLOCATOR;
```

io.netty.buffer.ByteBufUtil

```java
static final ByteBufAllocator DEFAULT_ALLOCATOR;
    static {
        //以io.netty.allocator.type为判断标准，没有的话，安卓平台用非池化实现，其他用池化实现。
        String allocType = SystemPropertyUtil.get(
                "io.netty.allocator.type", PlatformDependent.isAndroid() ? "unpooled" : "pooled");
        allocType = allocType.toLowerCase(Locale.US).trim();
        ByteBufAllocator alloc;
        if ("unpooled".equals(allocType)) {
            alloc = UnpooledByteBufAllocator.DEFAULT;
            logger.debug("-Dio.netty.allocator.type: {}", allocType);
        } else if ("pooled".equals(allocType)) {
            alloc = PooledByteBufAllocator.DEFAULT;
            logger.debug("-Dio.netty.allocator.type: {}", allocType);
        } else {
            //io.netty.allocator.type设置的不是"unpooled"或者"pooled"，就用池化实现。
            alloc = PooledByteBufAllocator.DEFAULT;
            logger.debug("-Dio.netty.allocator.type: pooled (unknown: {})", allocType);
        }
        DEFAULT_ALLOCATOR = alloc;
        THREAD_LOCAL_BUFFER_SIZE = SystemPropertyUtil.getInt("io.netty.threadLocalDirectBufferSize", 0);
        logger.debug("-Dio.netty.threadLocalDirectBufferSize: {}", THREAD_LOCAL_BUFFER_SIZE);
        MAX_CHAR_BUFFER_SIZE = SystemPropertyUtil.getInt("io.netty.maxThreadLocalCharBufferSize", 16 * 1024);
        logger.debug("-Dio.netty.maxThreadLocalCharBufferSize: {}", MAX_CHAR_BUFFER_SIZE);
    }
```

### **两种设置池化实现的方式**

```java
.childOption(ChannelOption.ALLOCATOR, UnpooledByteBufAllocator.DEFAULT)
 String allocType = SystemPropertyUtil.get(
                "io.netty.allocator.type", PlatformDependent.isAndroid() ? "unpooled" : "pooled");
```

### **堆外内存的分配本质？内存池实现?**

io.netty.buffer.PooledDirectByteBuf

```java
private static final 
    //Recycler就是Netty的轻量级内存池的实现
    Recycler<PooledDirectByteBuf> RECYCLER = new Recycler<PooledDirectByteBuf>() {
        @Override
        protected PooledDirectByteBuf newObject(Handle<PooledDirectByteBuf> handle) {
            return new PooledDirectByteBuf(handle, 0);
        }
    };
    //从“池”里借一个用
    static PooledDirectByteBuf newInstance(int maxCapacity) {
        PooledDirectByteBuf buf = RECYCLER.get();
        buf.reuse(maxCapacity);
        return buf;
    }
```

io.netty.util.Recycler 从池中取对象

```java
public final T get() {
    if (maxCapacityPerThread == 0) {
        //表明没有开启池化
        return newObject((Handle<T>) NOOP_HANDLE);
    }
    Stack<T> stack = threadLocal.get();
    DefaultHandle<T> handle = stack.pop();
    //试图从“池”中取出一个，没有就新建一个
    if (handle == null) {
        handle = stack.newHandle();
        handle.value = newObject(handle);
    }
    return (T) handle.value;
}
```

io.netty.buffer.PooledByteBuf 还对象到池中

```java
//归还对象到“池”里去，pipeline的tail会调用
@Override
protected final void deallocate() {
    if (handle >= 0) {
        final long handle = this.handle;
        this.handle = -1;
        memory = null;
        chunk.arena.free(chunk, tmpNioBuf, handle, maxLength, cache);
        tmpNioBuf = null;
        chunk = null;
        recycle();
    }
}
```

io.netty.util.Recycler的default实现，具体写明如何还回到内存池中

```java
static final class DefaultHandle<T> implements Handle<T> {
    ......
    public void recycle(Object object) {
        if (object != value) {
            throw new IllegalArgumentException("object does not belong to handle");
        }

        Stack<?> stack = this.stack;
        if (lastRecycledId != recycleId || stack == null) {
            throw new IllegalStateException("recycled already");
        }
        //释放用完的对象到池里面去
        stack.push(this);
    }
}
```

### **怎么从堆外内存切换堆内使用？**

io.netty.util.internal.PlatformDependent

```text
// We should always prefer direct buffers by default if we can use a Cleaner to release direct buffers.
//使用堆外内存两个条件：1 有cleaner方法去释放堆外内存； 2 io.netty.noPreferDirect 不能设置为true
DIRECT_BUFFER_PREFERRED = CLEANER != NOOP
    && !SystemPropertyUtil.getBoolean("io.netty.noPreferDirect", false);
if (logger.isDebugEnabled()) {
    logger.debug("-Dio.netty.noPreferDirect: {}", !DIRECT_BUFFER_PREFERRED);
}
```

- 方法 1：参数设置 io.netty.noPreferDirect = true;
- 方法 2：传入构造参数false
  ServerBootstrap serverBootStrap = new ServerBootstrap();
  UnpooledByteBufAllocator unpooledByteBufAllocator = new UnpooledByteBufAllocator(false); //不推荐
  serverBootStrap.childOption(ChannelOption.ALLOCATOR, unpooledByteBufAllocator)

### **堆外内存的分配本质？**

首先在io.netty.buffer.UnpooledByteBufAllocator会先进行判断，是否有hasUnsafe()方法和是否有noCleaner()方法。

```java
 protected ByteBuf newDirectBuffer(int initialCapacity, int maxCapacity) {
        final ByteBuf buf;
        if (PlatformDependent.hasUnsafe()) {
            buf = noCleaner ? new InstrumentedUnpooledUnsafeNoCleanerDirectByteBuf(this, initialCapacity, maxCapacity) :
                    //大多情况：
                    new InstrumentedUnpooledUnsafeDirectByteBuf(this, initialCapacity, maxCapacity);
        } else {
            buf = new InstrumentedUnpooledDirectByteBuf(this, initialCapacity, maxCapacity);
        }
        return disableLeakDetector ? buf : toLeakAwareBuffer(buf);
    }
```

io.netty.buffer.UnpooledDirectByteBuf#allocateDirect，最终实际调用的是JDK的方法
